import mlflow
import torch
from src.configs.config import TrainingConfig, PathConfig
from src.data.datamodule import ChestDataModule
from src.interpretability.evaluation import MetricsReporter, evaluate_model

import structlog

from src.utils.helpers import load_best_model

# Configure structlog
structlog.configure(
    processors=[
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.TimeStamper(),
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.PrintLoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


def main():
    # Instantiate configs
    train_config = TrainingConfig()
    path_config = PathConfig()

    # Prepare device
    device = torch.device(
        "mps"
        if torch.backends.mps.is_available()
        else "cuda" if torch.cuda.is_available() else "cpu"
    )

    logger.info("Using device", device=device)

    # DataModule
    data_module = ChestDataModule(train_config, path_config)
    data_module.train_dataloader()
    data_module.val_dataloader()
    test_loader = data_module.test_dataloader()

    loaded_model = load_best_model(
        experiment_name="ChestXRay",
        metric="val_loss",
        tracking_uri="http://localhost:5000",
    )

    # Evaluate on the test set
    y_true, y_prob = evaluate_model(loaded_model, test_loader, device)

    # Generate final metrics
    with mlflow.start_run(run_name="model_evaluation"):
        reporter = MetricsReporter()
        reporter.calculate_metrics(y_true, y_prob)
        reporter.log_to_mlflow()
        print("Test Macro ROC AUC:", reporter.metrics["macro_roc_auc"])
        print("Test Weighted ROC AUC:", reporter.metrics["weighted_roc_auc"])


if __name__ == "__main__":
    main()
